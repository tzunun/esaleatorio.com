---
title: Brain probably is a computer, whatever that means – Aeon Essays 
date: 2019-05-22 
draft: false 
---

Story source:

https://aeon.co/essays/your-brain-probably-is-a-computer-whatever-that-means


‘The brain is a computer’ – this claim is as central to our scientific
understanding of the mind as it is baffling to anyone who hears it. We are
either told that this claim is just a metaphor or that it is in fact a
precise, well-understood hypothesis. But it’s neither. We have clear reasons
to think that it’s _literally true_ that the brain is a computer, yet we don’t
have any clear understanding of what this _means_. That’s a common story in
science.

To get the obvious out of the way: your brain is not made up of silicon chips
and transistors. It doesn’t have separable components that are analogous to
your computer’s hard drive, random-access memory (RAM) and central processing
unit (CPU). But none of these things are essential to something’s being a
computer. In fact, we don’t know what is essential for the brain to be a
computer. Still, it is almost certainly true that it is one.

I expect that most who have heard the claim ‘the brain is a computer’ assume
it is a _metaphor_. The mind is a computer just as the world is an oyster,
love is a battlefield, and this shit is bananas (which has a metaphor inside a
metaphor). Typically, metaphors are literally false. The world isn’t – as a
matter of hard, humourless, scientific fact – an oyster. We don’t value
metaphors because they are _true_ ; we value them, roughly, because they
provide very suggestive ways of looking at things. Metaphors bring certain
things to your attention (bring them ‘to light’), they encourage certain
associations (‘trains of thought’), and they can help coordinate and unify
people (they are ‘rallying cries’). But it is nearly impossible to ever say in
a complete and _literally true way_ what it is that someone is trying to
convey with a _literally false metaphor_. To what, exactly, is the metaphor
supposed to turn our attention? What associations is the metaphor supposed to
encourage? What are we all agreeing on when we all embrace a metaphor?

If it were a metaphor to say that the brain is a computer, then we would
expect the claim to be literally false. This checks out with the point that
our brains aren’t organised, like PCs, into silicon-based hard drives, RAMs
and CPUs. We would also expect it to be difficult to flesh out exactly what we
mean when we say that the brain is a computer. The value of the claim, were it
a metaphor, would have to lie in whether it suggests the right things to
attend to, whether it calls to mind fruitful associations, and whether it
succeeds in bringing some coordination to the cognitive sciences.
[Some](https://perswww.kuleuven.be/~u0084530/doc/metaphors.html) think that
the supposed metaphor succeeds on these counts, while
[others](https://aeon.co/essays/your-brain-does-not-process-information-and-
it-is-not-a-computer) think it fails and has poisoned the well of cognitive-
science research.

But the claim that the brain is a computer is not just a metaphor. The
cognitive sciences are full of hypotheses to the effect that the brain
computes such-and-such in so-and-so a way. Many of our perceptual capacities,
for example, are understood in computational terms, and there aren’t any
viable alternatives around. Here are a handful of widely accepted hypotheses
about what the brain computes, though I will leave out the details:

  * Direction of sound from interaural time difference: if a loud crash occurs directly in front of you, its soundwaves will reach your left and right ears at the same time. If it occurs to your left, its soundwaves will reach your left ear slightly before they reach your right ear. When you hear a loud crash as being to your left or your right, your auditory system is computing, according to trigonometric principles, an estimate of that crash’s direction on the basis of the difference in times between when the sound waves arrived at your right and your left ears.
  * Depth from disparity (or stereopsis): most things reflect light to both your eyes. Take one of your fingers and hold it arm’s length away from you, and take another finger and hold it halfway between the farther finger and your face. Now fix your gaze on the closer finger. Your _farther_ finger will reflect light to a different part of your left eye (relative to its centre) than it will to your right eye (relative to _its_ centre). To see this, keep fixating on your closer finger. Close one eye and pay attention to the space that’s visible between your nearer and farther finger. Now switch the eyes – open one and close the other. You’ll notice that the visible space between your fingers is different. If you now bring your farther finger a bit nearer to you and repeat the eye-closing experiment, the effect is less dramatic. When you see one thing as twice as far away as another, part of what is happening is that your visual system computes an estimate of depth by first computing which retinal cells are responding to the same point in the world, and then determining the relative difference (‘disparity’) in the positions or coordinates of those retinal cells (greater disparities = greater depth).
  * Contour integration: when looking at the outline shape of an object in a cluttered scene, your visual system initially registers a bunch of tiny, individual segments of lines or contours (imagine lots of dashed lines). The visual system has to determine which line segments go with each other – which segments are parts of a common object’s outline, and which belong to different ones. The visual system computes outlines from line segments on the basis of, among other things, how close together those segments are, how similar in orientation they are, and whether they form an approximately straight line.
  * Surface colour from illumination: the light that reaches your eye from a surface is a product of that surface’s colour and the colour of the illumination. So, your white shoes will reflect different types of light depending on whether it is daytime or dusk, or whether you are on the dance floor or in a fluorescent-lit bathroom. Still, you can usually tell that your shoes are _white_ under these different conditions. When you see something as having a certain colour, your visual system is computing an estimate of the object’s colour by taking into account the nature of the illumination. The reason some people saw that dress as blue and black, and others saw it as white and gold, is that their visual systems are computing colours from different estimates of what the illumination is like.

Progress in cognitive science regularly consists in saying with mathematical
precision exactly what is being computed – what direction should be estimated
from some interaural time difference? – and exactly how the computation is
performed. Hypotheses concerning these details can be and are tested against
experimental observations, both of how people perform on tests (point to the
loud noise, please) and of how populations of neurons respond to stimuli.
There’s pretty stable agreement about what would count as evidence for or
against hypotheses of this sort. Nobody has any real idea of how else to
understand our abilities to, for example, perceive the locations of sounds or
the depths, outlines and colours of objects.

Cognitive scientists seem to speak literally when they say that the brain
computes

That’s a level of clarity and commitment to a premise that is uncharacteristic
of metaphorical claims. Typically, if you press someone to explain just what
type of oyster the world is, and where on Earth its gills are, everybody will
agree that you’ve missed the point. But in cognitive science, hypothesising
what specific kinds of computations the brain carries out and devising
experiments to test these hypotheses is, far from missing the point, very
often the _job description_.

Cognitive scientists seem to speak literally when they propose that the brain
computes a certain function in a certain way. And the claim that the brain
literally computes _something_ implies that the brain is literally a computer
(at least in part).

If it were just a metaphor to say that the brain is a computer, then we would
expect it to be forever unclear what literal truths we are trying to convey
with the use of this (presumably literally false) metaphor. But if the claim
is meant literally, then we should hope for some precise account of what one
is saying when one says that the brain computes.
[Some](https://medium.com/the-spike/how-to-find-out-if-your-brain-is-a-
computer-644a1a4fed1b) have dug in their heels, confident that cognitive
scientists _do_ know just what they are saying when they say that brains are
computers: the claim is in fact a concrete, well-understood scientific
[hypothesis](https://medium.com/the-spike/yes-the-brain-is-a-
computer-11f630cad736).

Unlike metaphors, whose value is independent of their literal falsity,
scientific hypotheses are valuable insofar as they are (likely close to being)
_true_ and offer concrete, insightful explanations of phenomena. And, unlike
metaphors, ideally one should be able to state precisely and with some
completeness what a scientific hypothesis says about the world, and what
experimental observations it predicts (give or take some background
assumptions).

But the suggestion that we have on hand a precise and well-understood claim
when we say that the brain is a computer feels forced. There really isn’t much
clarity or agreement at all on what it means to say that the brain is a
computer, even among those who agree that the claim is meant _literally_.

There is a whole mathematical field dedicated to understanding computation.
Yet you might be surprised about how little clarity there is about what
computation _is_. Foundational to the field is the Church-Turing thesis, named
after the American philosopher-logician Alonzo Church (1903-95) and the
English logician-mathematician Alan Turing (1912-54). Both Church and Turing
were working to figure out what sorts of mathematical problems can and cannot
be solved by blindly following rules of the sort you learn in order to do long
division, long multiplication, or to solve quadratic formulas. In other words:
what mathematical problems can be solved by rule-based calculation or, as it
was called back then, _computation_? To answer this question, both Church and
Turing developed, around the same time in the 1930s, mathematical models to
capture what happens when one calculates the solution to a problem.

Church’s model of computation, the ‘lambda calculus’, emphasises giving
explicit definitions of functions in terms of a small stock of very basic
functions. To give a rough idea, the function 3 + (14 – _x_ ) can be defined
as a sum, 3 + _y_ , where y itself is defined as a subtraction, 14 – _x_. To
determine what 3 + (14 – _x_ ) equals, when _x_ = 6, one first solves for 14 –
6, and then adds that solution to 3, giving 11. The idea is to break up the
problem into its smallest chunks, solving it chunk by chunk. Church proposed
that a mathematical problem is computable just in case it can be expressed
within his sparse language for defining functions in terms of more basic
functions. For him, problemsolving involves giving an explicit definition of
the problem, and then following that definition to its conclusion.

Turing’s model of computation emphasises rote rule-following. Remember how,
when doing long division or solving for _x_ , you would write one line and
then, following a rule you were taught, you would write down another line
underneath it that was based on the previous line, and so on until the last
line you wrote gave the solution? Sometimes we felt entitled to skip a step or
two, but the teacher would insist that you must ‘Show your work!’ Turing took
this to the extreme. He defined a system of rules or constraints for writing
one line based on a previous line, in which you could see _all the work_ down
to the last, minute detail, so that your teacher could not credibly ask you to
show any more.

Suppose you start with a line of symbols expressing the problem to be solved:

> … 21 x 3 = ? …

and you need to get to the final line expressing the solution:

> … 63 …

Imagine you are writing everything down on graph paper, with no more than one
item or symbol per square. You have an instruction manual telling you what to
do at each step of the computation. You always start at a particular line, in
a particular square on your notebook, and you handle only one square per step.
Steps in the instruction manual say that, if the current square with which you
are dealing contains such-and-such, then either erase that item from the
square, leaving a blank spot in its place:

> … 21 x 3 = [?] …

> … 21 x 3 = [ ] … (where ‘[ ]’ indicates the square in the notebook with
which you are dealing)

or enter so-and-so into the square (if it is blank):

> … 21 x 3 = [ ] …

> … 21 x 3 = [6] …

or move to an adjacent square:

> … 21 x 3 = [6] …

> … 21 x 3 = 6 [ ] …

Turing’s idea was that nobody who watches a solution get churned out in this
way could reasonably ask for more work to be shown. These rules or constraints
on how to solve a problem define what is called a ‘Turing machine’: a system
that shows _all_ its work. The term ‘Turing machine’ refers first and foremost
to an abstract mathematical object or system, a set of constraints on how one
line might be transformed into another, rather than to the physical devices
that we build to embody these constraints. Turing proposed that a mathematical
problem is computable just in case the expression of that problem can be
transformed into an expression of its solution under these constraints.

Miraculously, the set of mathematical problems that can be defined and solved
in Church’s language turns out to be exactly the same set of problems that can
be solved by a Turing machine. Bolstered by this convergence, the Church-
Turing thesis is that the set of problems that can be computed are just the
set of problems that can be solved either by a Turing machine or that can be
defined within Church’s calculus. In fact, there are many models besides
Church’s and Turing’s, including certain types of artificial neural networks,
that are capable of giving solutions to exactly the same set of problems.

Church-Turing tells us which problems are _computable_ , but not what
_computation_ is

One of Turing’s great achievements was to show how a single ‘universal Turing
Machine’ could be programmed to perform different computations. For any given
problem, one can design a Turing machine with a custom-crafted instruction
manual for solving that problem. Turing went a step further and showed how to
make a Prime Mover of a machine, the instruction manual of which could guide
you in performing any computation, so long as you tell it which one. This
system performs a computation that computes other computations. This
convenient innovation is the seed of the modern, multipurpose computers we
have today, on which we install and uninstall all sorts of programs. But
there’s no reason that a computer _has_ to be programmable in this way. Some
jobs (including, maybe, estimating the direction of a sound from interaural
time differences) require computing the solution to only one kind of problem.
Church, Turing and others provided an exciting variety of ways to get
solutions to the same set of problems.

The Church-Turing thesis tells us which problems are _computable_ , but not
what _computation_ is. Church and Turing offered different models for solving
problems. They got the same solutions to the same problems but by different
means. That means that you don’t have to be a Turing Machine to compute. You
don’t have to write down line after line, step after step, in just the way
that Turing describes. You could go to Church instead. You could turn to any
of the other models of computation that give the same solutions to the same
problems but in different ways. What makes these all models of _computation_?
And how do we understand the possibility that two different models could both
capture the _same_ computation – long division, say?

Even if we focus on just one model of computation, there will be many
different ways to get the same solution to a problem. But if a solution is
reached through wildly different steps, then intuitively different
computations have been carried out. This matters tremendously for cognitive
science, which cares not just about saying what computable problem the brain
might be solving but also about what specific kind of procedure or computation
the brain performs in solving that problem. Different computations can take
different amounts of time, do things in different orders according to
different causal chains, and break down in different ways (therefore calling
for different medical treatments when treatment is needed).

The Church-Turing thesis is just silent about deeply important features of
computations. It says what set of problems can be computed and it says this by
reference to two models of computation. But the thesis doesn’t say what it is
about the different models of computation that makes them all models of
_computation_. It doesn’t say what makes two problemsolving methods ‘the same’
computation or ‘different’ computations. So it says nothing about _what a
computation is_. (Turing did try to give an account of computation, but it is
debatable how successful it is. The American mathematician Richard Shore
worried that addressing these issues might be pie in the sky.) Many roads lead
to Rome. The Church-Turing thesis told us the location of Rome, but not what
makes something a road.

We haven’t even touched the main sources of difficulty. The mathematical
theory of computation is just that: mathematics. But the brain, like your PC,
is a concrete thing. It’s a piece of nature. The stuff of mathematics doesn’t
break down or decay. Numbers don’t chip at the edges; 1 + 2 will never take on
a different value because moss started growing over the number 2 or because
the operation of addition got jammed. Even if we had a precise, adequate and
agreed-upon _mathematical_ definition of what computation is (and we don’t),
this wouldn’t tell us what it means for a part of the natural world – the sort
of thing that can rust or sneeze – to perform a computation. One problem is to
understand the nature of computation. Another is to understand the nature of
computation _in nature._

In fact, we often say that something in nature performs a particular
computation even when that thing doesn’t always give out the answers that the
computation, by definition, is supposed to give out. My laptop runs the latest
version of my word processor even though it will sometimes stall on account of
the 732 tabs that I have open on my web browser. A [Lego
set](https://www.youtube.com/watch?v=FTSAiF9AHN4) can compute addition even if
some of the pieces get gunk in them, preventing them from moving as they
should. Likewise, your brain can compute the direction from which a sound
arises according to a particular trigonometric rule, even if it sometimes
makes errors that are incompatible with that rule. We _expect_ these errors in
anyone, and we find greater errors among those of us who are hearing-impaired.
But why think that most people’s auditory systems are solving the same
trigonometric problems in error-prone ways rather than solving their own
personally tailored problems in an error-free way?

It isn’t as arbitrary as it seems. The reasonable thing to say is that
something performs a computation if it _would_ _normally_ perform that
computation. Abnormal and non-ideal circumstances – having a head cold while
walking through an organ-grinding festival – can lead to errors. This causes
surprisingly few difficulties in cognitive science. Cognitive scientists are
able to identify, in testable and motivated ways, which behaviours are the
result of abnormal or interfering factors. But even though we have a stable,
usable empirical concept of what computation a system ‘would normally
perform’, it is wildly unsettled what it actually _is_ for something to
‘normally perform’ one computation rather than another.

That’s just the beginning of our problems. In saying which problems could be
solved by rule-bound calculations, Church and Turing defined models in which a
set of symbols expressing the problem (the line ‘21 x 3 = ?’ in your notebook)
is transformed into a set of symbols expressing the solution (‘63’). But the
very same models could be used to convert meaningless gibberish (‘j&^@’) into
other meaningless gibberish (‘•¡ø∂’). Would those transformations then count
as computations? Turning back to the natural world: does it really matter that
the firing of a neuron means that a sound came from _thataway_ , or does it
just matter that when this neuron here starts firing ( _whatever_ news or
information that neuron might have about the world), it triggers those neurons
over there to start firing?

If our brains are computers, our mental lives must be rich with meaningful
information

In fact, the question of whether computation involves meaningful
representation can bear on the question of what it is for a system to
‘normally perform’ a computation. If the Arabic numerals ‘1’, ‘2’, ‘3’, …
represent the numbers one, two, three and so on, and if some machine or
organism takes any two of those numerals and typically churns out a numeral
denoting its sum, then there’s good reason to think that this thing normally
performs addition, even if it occasionally hiccups out the wrong sum when it
is tired or distracted. If the system is just swishing around meaningless
symbols, it is much more difficult to say what should determine that the
system normally performs one computation rather than another one.

Many have thought that computation must involve meaningful representation or
information. Far from saying that we are meaningless machines, the claim that
our brains are computers would require that our mental lives are rich with
meaningful information. Yet if computation requires giving meaningful answers
to meaningful problems then, in order to say what it would be for the brain to
be a computer, one must also say what it would be for activity in the brain to
be meaningful. One difficult question begging another. There are a number of
[dissenting](http://philosophyofbrains.com/2015/08/11/does-computation-
require-representation.aspx) and
[intermediate](https://bjpsbooks.wordpress.com/2016/09/07/gualtiero-piccinini-
physical-computation/) positions about whether computations must be
meaningful.

It’s not so clear that there is any single answer to what a computer is.
Computation might not be a uniform kind of thing in nature. Many things are
said to compute: household PCs, [analogue machines](https://medium.com/the-
spike/brains-as-analog-computers-fa297021f935),
[looms](https://www.theatlantic.com/technology/archive/2014/09/before-
computers-people-programmed-looms/380163/), brains and even
[DNA](https://www.technologyreview.com/s/534721/what-can-dna-based-computers-
do/) and quantum-mechanical systems. These are quite different pieces of
nature (or, in the case of PCs, looms and analogue machines, human artifice –
itself a piece of nature). What it is for one sort of thing to compute might
turn out to be quite different from what it is for another sort of thing to
compute. The features of a [Navy](https://www.youtube.com/watch?v=s1i-dnAH9Y4)
fire-control system that make it distinctively computational might differ from
the features of the brain that make it distinctively computational. It gets
worse: the features of single neurons or networks of neurons that make them
distinctively computational might differ from the more high-level features of
our mental capacities – our abilities to perceive the direction of a sound
source or the depth of a surface – that make those capacities distinctively
computational.

There is just no consensus on what it is to be a computation or for a piece of
nature to perform a computation. Neither question might have a _general_
answer. The common claim that the brain is a computer is not, at the moment, a
concrete, precise, well-understood scientific hypothesis.

Still, the claim is almost certainly true.

Earlier I said that the cognitive sciences are full of concrete, well-
understood hypotheses that the brain computes such-and-such in so-and-so way.
Now I am saying that the claim that the brain computes is not a concrete,
well-understood hypothesis. Worse, I am still insisting that the claim is
almost certainly true. Bananas?

The concrete, well-understood hypotheses are the _specific_ hypotheses that
the brain performs _specific computations_. Many of these hypotheses have lots
of evidence. Each of these specific hypotheses entails that the _general_
claim that the brain computes, full stop, must be true. (Note: they do _not_
imply that the brain _only_ computes! It certainly does other things.) The
thing is, none of this means that the general claim that the brain computes is
itself a concrete and well-understood hypothesis. The hypothesis that the
brain computes X can yield testable predictions and insightful explanations.
The claim that the brain computes, full stop, doesn’t on its own do much
predicting or explaining at all – or at least, I suspect, not nearly so much
as is advertised.

Many research articles begin, in the introduction, with: ‘We are interested in
whether the brain computes X,’ and then: ‘If it does compute X, then upon
conducting such-and-such experiment we should see this-or-that result.’ And
while scientists might disagree about whether the results are robust or
whether the controls are adequate, and they might suggest additional
experiments, there is rarely fundamental disagreement about what sorts of
experiments are relevant to the hypothesis. It’s not so with: ‘We are
interested in whether the brain computes.’ This naturally suggests… what?
There’s not been wide agreement about what experiments one should run to test
this general hypothesis or even about what phenomena this general hypothesis
is supposed to explain. While the statement ‘the-brain-computes-X’, _treated
as a chunk_ , might be a perfectly good, well-understood scientific
hypothesis, it will take a lot of work to say exactly what the crucial part of
the sentence ‘The brain computes…’ says all by itself.

The claim that the brain is a computer is not merely a metaphor – but it is
not quite a concrete hypothesis. It is a [theoretical
framework](https://perswww.kuleuven.be/~u0084530/doc/marr_levels.html) that
[guides fruitful
research](https://www.nytimes.com/2015/06/28/opinion/sunday/face-it-your-
brain-is-a-computer.html). The claim offers to us the general formula ‘The
brain computes X’ as a key to understanding certain aspects of our astonishing
mental capacities. By filling in the X, we get mathematically precise,
testable and, in a number of cases, highly supported hypotheses about how
certain mental capacities work. So, the defining claim of the theoretical
framework, that the brain computes, is almost certainly true.

Though we are in a position to say that it is likely true that the brain is a
computer, we do not yet have any settled idea of what it means for the brain
to be a computer. Like a metaphor, it really is unclear what the literal
message of the claim is. But, unlike a metaphor, the claim is intended to be a
true one, and we do seek to uncover its literal message.

Even if you agree to the literal claim that the brain computes since, after
all, our best theories hold that it computes X, Y and Z, you might be unsure
or disagree about what it is for something to be a computer. In fact, it looks
like a lot of people disagree about the meaning of something that they all
agree is true. The points of disagreement do not spoil the point of agreement.
You can have a compelling reason to say that a claim is true even before you
light upon a clear understanding of what that claim means about the world.

Often, we get our hooks into some general phenomenon and hold on for dear
science

This position isn’t so uncommon in science. The concept of
[temperature](https://global.oup.com/academic/product/inventing-
temperature-9780195337389?cc=be&lang=en&) could be put to good scientific
service even before we had any real conception of what it was – a conception
that was developed through a lot of labour and ingenuity. In the early 1960s,
the American physicist Richard Feynman said that it ‘is important to realise
that in physics today, we have no knowledge of what energy _is_ ’, and it’s
[debated](https://cen.acs.org/articles/89/i46/Explaining-Energy.html) how much
better off we are now. And while Albert Einstein offered a detailed theory of
gravity, we don’t have a full-enough understanding of what gravity is to
reconcile it with the other fundamental forces. See also: [quantum
mechanics](https://inference-review.com/article/to-be-not-to-be-maybe) (I
won’t say any more than that because I don’t know any more). Scientific
hypotheses have often yielded good predictions and insightful explanations,
often by applying mathematical principles to natural systems, even though,
when it comes to saying what it really means for the world to be as the
hypothesis says… that takes some figuring-out.

Throughout the history of science, we have often started by knowing just
enough to be able to point to an important thing, though we didn’t grok it
well enough to paint the full picture. We do not need to start with a fixed,
fully fleshed-out conception of some deep feature of the world and then see
whether it fits our observations. Often, we get our hooks into some general
phenomenon as we try to solve specific problems, and we have to drag ourselves
closer and closer to the phenomenon underlying those problems, developing our
understanding over the course of generations, sometimes withstanding fierce
swings and great setbacks as we hold on for dear science. We’re on to
something when we say the brain is a computer, but it might not be clear for a
while what exactly it is that we’re on to.

How will we come to make sense of the general claim that the brain is a
computer? I think we have to resist talking solely in abstractions. We have to
keep ourselves firmly rooted in the _specific_ hypotheses that the brain
computes X, Y and Z. What do these specific hypotheses have in common? What
makes them tick? How do they give rise to systematic predictions and why do
they seem to provide insightful explanations? What would we lose if, instead
of saying that the brain _computes_ depth from binocular disparity, we adopted
a totally undefined term, ‘jorgmute’ (I know not what it is to ‘jorgmute’),
and said that the brain _jorgmutes_ depth from binocular disparity?

These questions are, in fact, just the sorts of questions you find
philosophers of science regularly asking. To its credit, cognitive science
(unlike [other
sciences](https://www.scientificamerican.com/article/physicists-are-
philosophers-too/), recently) has always welcomed and acknowledged the
contributions of philosophers. As the late American philosopher Jerry Fodor
wrote in _The Language of Thought_ (1975):

> One wants to say: ‘If our psychology is, in general, right then the nature
of the mind must be, roughly, this …’ and then ﬁll in the blank. … [T]he
experimentalist can work the other way around: ‘If the nature of the mind is
roughly …, then our psychology ought henceforth to look like this: …’, where
this blank is filled by new first-order theories. We ascend, in science, by
tugging one another’s bootstraps.

The brain is almost certainly partly a computer. We still have to uncover what
that means about our brains and about ourselves.

